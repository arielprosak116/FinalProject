{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The Revenge of Rocchio's Angels",
   "id": "c6962dbc7032e16e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will be #1 this time",
   "id": "dd8b17b5e7537cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Local Script Dependencies",
   "id": "a330a05fa432fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:12:06.033433100Z",
     "start_time": "2026-01-13T21:11:49.243195500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from engine import SearchEngine\n",
    "from evaluate_map import *\n",
    "from optimizing import Optimize"
   ],
   "id": "6da5113226db22bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariel\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Full (Current) Pipeline",
   "id": "156f760f8fb4962f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T19:33:56.172910Z",
     "start_time": "2026-01-13T19:33:55.152578700Z"
    }
   },
   "cell_type": "code",
   "source": "se = SearchEngine()",
   "id": "4ae9d273b58a4a20",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T19:34:20.077640600Z",
     "start_time": "2026-01-13T19:34:20.030184700Z"
    }
   },
   "cell_type": "code",
   "source": "se.set_searcher(approach=\"bm25\",fb_terms=20, fb_docs=5, original_query_weight=0.6, mu=300) #MAP=0.264891",
   "id": "a9c52e26da8a6ef1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T19:34:21.899830600Z",
     "start_time": "2026-01-13T19:34:21.845186900Z"
    }
   },
   "cell_type": "code",
   "source": "topics = load_topics(\"Data/queriesROBUST.txt\")",
   "id": "aeaef1d3db21a4c2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T21:07:44.098350300Z",
     "start_time": "2026-01-13T19:34:23.513142700Z"
    }
   },
   "cell_type": "code",
   "source": "se.search_all_queries(topics, k=1000, m=100, output_file=\"Results/hey05.txt\", fusion_weight=0.5)",
   "id": "b499b678e7e8a315",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching topics:   0%|          | 1/249 [01:03<4:23:30, 63.75s/it]C:\\Users\\ariel\\PycharmProjects\\FinalProject\\engine.py:45: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return max(scores)*max_weight + (1-max_weight)*(sum(scores)-max(scores))/(len(scores)-1)\n",
      "Searching topics:  13%|█▎        | 32/249 [1:33:11<10:31:55, 174.72s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mse\u001B[49m\u001B[43m.\u001B[49m\u001B[43msearch_all_queries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtopics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mResults/hey05.txt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfusion_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\engine.py:130\u001B[39m, in \u001B[36mSearchEngine.search_all_queries\u001B[39m\u001B[34m(self, topics, k, run_tag, output_file, m, fusion_weight)\u001B[39m\n\u001B[32m    127\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m qid, query \u001B[38;5;129;01min\u001B[39;00m tqdm(topics.items(), desc=\u001B[33m\"\u001B[39m\u001B[33mSearching topics\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msearch_and_write_trec_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_tag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfusion_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfusion_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\engine.py:107\u001B[39m, in \u001B[36mSearchEngine.search_and_write_trec_run\u001B[39m\u001B[34m(self, query, k, topic_id, run_tag, output_file, m, fusion_weight)\u001B[39m\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msearch_and_write_trec_run\u001B[39m(\u001B[38;5;28mself\u001B[39m, query, k, topic_id, run_tag, output_file, m=\u001B[32m100\u001B[39m, fusion_weight=\u001B[32m0\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     hits = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mretrieve_rerank\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfusion_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    108\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(output_file, \u001B[33m\"\u001B[39m\u001B[33ma\u001B[39m\u001B[33m\"\u001B[39m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m    109\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m rank, hit \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(hits, start=\u001B[32m1\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\engine.py:100\u001B[39m, in \u001B[36mSearchEngine.retrieve_rerank\u001B[39m\u001B[34m(self, query, k, m, fusion_weight)\u001B[39m\n\u001B[32m     98\u001B[39m top_m = hits[:m]\n\u001B[32m     99\u001B[39m passages_top_m = split_passages(top_m)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m top_m_reranked = \u001B[43mrerank\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpassages_top_m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m top_m_fused = [Hit(docid=doc_score_tuple[\u001B[32m0\u001B[39m], score=doc_score_tuple[\u001B[32m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m doc_score_tuple \u001B[38;5;129;01min\u001B[39;00m weighted_rrf_fuse([top_m_reranked, top_m], weights=[\u001B[32m1\u001B[39m-fusion_weight,fusion_weight])]\n\u001B[32m    102\u001B[39m all_docs_reranked = top_m_fused + hits[m:]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\engine.py:47\u001B[39m, in \u001B[36mrerank\u001B[39m\u001B[34m(query, retrieval_candidates, max_weight, reranker)\u001B[39m\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(scores)*max_weight + (\u001B[32m1\u001B[39m-max_weight)*(\u001B[38;5;28msum\u001B[39m(scores)-\u001B[38;5;28mmax\u001B[39m(scores))/(\u001B[38;5;28mlen\u001B[39m(scores)-\u001B[32m1\u001B[39m)\n\u001B[32m     46\u001B[39m pairs = [[query, _remove_whitespaces(doc.page_content)] \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m retrieval_candidates]\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m cross_scores = \u001B[43mreranker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpairs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m per_doc_scores = defaultdict(\u001B[38;5;28mlist\u001B[39m)\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m score, doc \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(cross_scores, retrieval_candidates):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\util.py:68\u001B[39m, in \u001B[36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     63\u001B[39m         kwargs.pop(deprecated_arg)\n\u001B[32m     64\u001B[39m         logger.warning(\n\u001B[32m     65\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThe CrossEncoder.predict `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdeprecated_arg\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` argument is deprecated and has no effect. It will be removed in a future version.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     66\u001B[39m         )\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:707\u001B[39m, in \u001B[36mCrossEncoder.predict\u001B[39m\u001B[34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor, device, pool, chunk_size)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc=\u001B[33m\"\u001B[39m\u001B[33mBatches\u001B[39m\u001B[33m\"\u001B[39m, disable=\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n\u001B[32m    706\u001B[39m     batch = sentences[start_index : start_index + batch_size]\n\u001B[32m--> \u001B[39m\u001B[32m707\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    708\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    710\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    711\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    712\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m     features.to(device)\n\u001B[32m    714\u001B[39m     model_predictions = \u001B[38;5;28mself\u001B[39m.model(**features, return_dict=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3073\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__call__\u001B[39m\u001B[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   3071\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._in_target_context_manager:\n\u001B[32m   3072\u001B[39m         \u001B[38;5;28mself\u001B[39m._switch_to_input_mode()\n\u001B[32m-> \u001B[39m\u001B[32m3073\u001B[39m     encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3074\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3075\u001B[39m     \u001B[38;5;28mself\u001B[39m._switch_to_target_mode()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3161\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._call_one\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   3156\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3157\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not match batch length of `text_pair`:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3158\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3159\u001B[39m         )\n\u001B[32m   3160\u001B[39m     batch_text_or_text_pairs = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[32m-> \u001B[39m\u001B[32m3161\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3163\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3165\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3166\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3168\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3169\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3170\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3172\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3173\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3178\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3179\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3180\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3181\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3182\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3183\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.encode_plus(\n\u001B[32m   3184\u001B[39m         text=text,\n\u001B[32m   3185\u001B[39m         text_pair=text_pair,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3203\u001B[39m         **kwargs,\n\u001B[32m   3204\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3362\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   3352\u001B[39m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[32m   3353\u001B[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001B[38;5;28mself\u001B[39m._get_padding_truncation_strategies(\n\u001B[32m   3354\u001B[39m     padding=padding,\n\u001B[32m   3355\u001B[39m     truncation=truncation,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3359\u001B[39m     **kwargs,\n\u001B[32m   3360\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3362\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3363\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3364\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3366\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3367\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3368\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3369\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3372\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3373\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3374\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3375\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3376\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3377\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3378\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3379\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3380\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3381\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3382\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FinalProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:553\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[39m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001B[32m    551\u001B[39m     \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001B[32m--> \u001B[39m\u001B[32m553\u001B[39m encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[32m    560\u001B[39m \u001B[38;5;66;03m# `Tokens` has type: tuple[\u001B[39;00m\n\u001B[32m    561\u001B[39m \u001B[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[32m    562\u001B[39m \u001B[38;5;66;03m#                       list[EncodingFast]\u001B[39;00m\n\u001B[32m    563\u001B[39m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[32m    565\u001B[39m tokens_and_encodings = [\n\u001B[32m    566\u001B[39m     \u001B[38;5;28mself\u001B[39m._convert_encoding(\n\u001B[32m    567\u001B[39m         encoding=encoding,\n\u001B[32m   (...)\u001B[39m\u001B[32m    576\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[32m    577\u001B[39m ]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T00:16:30.122130600Z",
     "start_time": "2026-01-11T00:16:29.899015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qrels = load_qrels(\"Data/qrels_50_Queries\")   # or \"qrel301.txt\"\n",
    "run   = load_run(\"Results/hey05.txt\")\n",
    "\n",
    "map_score, ap_by_q = mean_average_precision(qrels, run)\n",
    "map_score"
   ],
   "id": "b0c945b3095989bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2844271042048603"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Extract Train Set Results",
   "id": "4c0a56e0296e8a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "HARD_QUERIES =[309, 308, 338, 344, 348, 320, 328, 334, 303, 339] # From EDA",
   "id": "39ab38f64035afd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_hits = {}\n",
    "hard_hits = {}\n",
    "for i, (qid, topic) in enumerate(topics.items()):\n",
    "    results = se.get_top_k(topic, k=1000, clean=True)\n",
    "    all_hits[f\"{qid}_{topic}\"] = results\n",
    "    if int(qid) in HARD_QUERIES:\n",
    "        hard_hits[f\"{qid}_{topic}\"] = results\n",
    "    if i==49:\n",
    "        print(qid)\n",
    "        break\n",
    "\n",
    "import pickle\n",
    "with open(\"pkls/top1000_rm3_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_hits, f)\n",
    "with open(\"pkls/top1000_rm3_train_hard.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hard_hits, f)"
   ],
   "id": "960db8da077dbae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res = se.retrieve_rerank(\"international organized crime\", k=1000, m=50)",
   "id": "9174847dac4e70a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res",
   "id": "eaa14cfbaf2f32d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "# analyzer = get_lucene_analyzer(stemmer='porter', stopwords=False)\n",
    "# se.reader.get_term_counts(\"spanish\",analyzer) #(df,cf)"
   ],
   "id": "cbb07f9239d15fef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This one creates a file called \"run.txt\" with submitting format, can change file name\n",
    "se.search_all_queries(topics, k=5, m=2, output_file=\"Results/hey.txt\")"
   ],
   "id": "868c2ffeffd32b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qrels = load_qrels(\"Data/qrels_50_Queries\")   # or \"qrel301.txt\"\n",
    "run   = load_run(\"Results/run.txt\")\n",
    "\n",
    "map_score, ap_by_q = mean_average_precision(qrels, run)\n",
    "map_score"
   ],
   "id": "37c144d52d85f00d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stats = se.reader.stats()\n",
    "print(f\"average terms per doc: {stats['total_terms']/stats['documents']}\")"
   ],
   "id": "2103da6a327cd83e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "doc = se.searcher.doc(\"FT921-3160\")\n",
    "text = doc.raw()\n",
    "text"
   ],
   "id": "730d85da0d99e8fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "map = get_map_by_paths(\"Data/qrels_50_Queries\", \"Results/run.txt\")",
   "id": "af0ff13b1d2f05ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "opti = Optimize()\n",
    "# fb_terms_values = [5, 6, 8, 10, 15, 20]\n",
    "# fb_docs_values = [5, 7, 10, 15]\n",
    "# og_query_weight_values = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "mus = [200,300,400,500,600,700,800,900,1000,1100,1200]\n",
    "\n",
    "opti.optimize_qld(topics, [20], [5], [0.6], mus, k=1000)"
   ],
   "id": "c188f7e310540d19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
