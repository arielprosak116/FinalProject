import os
import ast
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from dotenv import load_dotenv
import re
from dataclasses import dataclass, field
from typing import Any, Dict, Optional, Tuple, List, Union
from pathlib import Path
import json


load_dotenv()
SPLITTER_ARGS = ast.literal_eval(os.getenv("SPLITTER_ARGS"))
SPLITTER_SINGLETON = RecursiveCharacterTextSplitter(**SPLITTER_ARGS)

# Generic SGML-ish block: <TAG ...> ... </TAG>
# TAG names: letters/digits/_/-
_BLOCK = re.compile(
    r"<(?P<tag>[A-Za-z][A-Za-z0-9_-]*)(?P<attrs>\s+[^>]*)?>\s*(?P<content>.*?)\s*</(?P=tag)>",
    re.DOTALL
)

# Remove any remaining tags (inline or otherwise)
_ANY_TAG = re.compile(r"</?[^>]+>")

# Common “annotation-like” remnants you may want to drop from body
_TEXT_MARKER = re.compile(r"^\s*\[Text\]\s*", re.IGNORECASE)

@dataclass(slots=True)
class Hit:
    docid: str
    score: float
    query: Optional[str] = None
    text: Optional[str] = None
    meta: Dict[str, Any] = field(default_factory=dict)

    def __getattr__(self, name: str):
        try:
            return self.meta[name]
        except KeyError:
            raise AttributeError(name)

    # For LangChain
    @property
    def page_content(self) -> str:
        return self.text

    @property
    def metadata(self) -> dict:
        return {"docid": self.docid, "query": self.query, **self.meta}



def create_llm_generated_queries(
    input_path: str | Path,
    out_paths: List[str | Path] | None = None,
    expected_cols: int = 3,
    sep: str = " ",
    encoding: str = "utf-8",
) -> Tuple[Path, ...]:
    """
    Read The query permutations generated by the llm:
        <qid>: option1, option2, option3, ...
    and write N separate TREC query files (one per option/"column"):
        qid<tab>option_i
    """
    input_path = Path(input_path)
    if out_paths is None:
        out_paths = [Path(f"queries_col{i+1}.txt") for i in range(expected_cols)]
    else:
        out_paths = [Path(p) for p in out_paths]

    columns: List[List[str]] = [[] for _ in range(expected_cols)]

    with input_path.open("r", encoding=encoding) as f:
        for line_no, raw in enumerate(f, start=1):
            line = raw.strip()
            if not line:
                continue

            if ":" not in line:
                raise ValueError(f"Malformed line {line_no}: missing ':' -> {line!r}")

            qid, rest = line.split(":", 1)
            qid = qid.strip()
            if not qid:
                raise ValueError(f"Malformed line {line_no}: empty qid -> {line!r}")

            options = [opt.strip() for opt in rest.split(",") if opt.strip() != ""]
            if len(options) != expected_cols:
                raise ValueError(
                    f"Line {line_no} (qid={qid}) has {len(options)} options, expected {expected_cols}: {line!r}"
                )

            for i in range(expected_cols):
                columns[i].append(f"{qid}{sep}{options[i]}")

    for out_path in out_paths:
        out_path.parent.mkdir(parents=True, exist_ok=True)

    for out_path, lines in zip(out_paths, columns):
        out_path.write_text("\n".join(lines) + ("\n" if lines else ""), encoding=encoding)

    return tuple(out_paths)




def write_topk_jsonl_query(hits, out_path, qid):
    """
    Appends one JSONL record:
      {"query": "<query or qid>", "hits": [{"docid": "...", "score": ...}, ...]}
    For retrieval checkpointing.
    """
    if not hits:
        return
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    query = hits[0].query
    seen = set()
    hit_list = []

    for h in hits:
        # defensive: ensure single-query invariant
        if h.query != query:
            raise ValueError("Hits contain multiple queries")

        if h.docid in seen:
            print("DUPLICATE (HOW?)")
            continue

        seen.add(h.docid)
        hit_list.append({
            "docid": str(h.docid),
            "score": float(h.score),
            "text": str(h.text),
        })

    rec = {
        "query": query,
        "qid": qid,
        "hits": hit_list,
    }

    with out_path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")


def iter_query_hits(jsonl_path: str | Path):
    """
    Stream a JSONL file line-by-line to yield previous resutls.
    """
    jsonl_path = Path(jsonl_path)

    with jsonl_path.open("r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue

            try:
                rec = json.loads(line)
            except json.JSONDecodeError as e:
                raise ValueError(f"Bad JSON on line {line_no} in {jsonl_path}") from e

            q = rec["query"]
            hits_raw = rec.get("hits", [])
            hits = [Hit(query=q, docid=str(h["docid"]), score=float(h["score"])) for h in hits_raw]

            yield rec["qid"], hits


def _normalize_ws(s: str) -> str:
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # Collapse spaces/tabs
    s = re.sub(r"[ \t]+", " ", s)
    # Collapse many blank lines
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def clean_inner_text(s: str) -> str:
    """
    Cleans text inside a tag:
    - strips any nested tags like <F P=105> ... </F>
    - removes [Text] marker (common in newswire)
    - normalizes whitespace
    """
    s = _ANY_TAG.sub("", s)  # drop nested tags
    s = _TEXT_MARKER.sub("", s)  # drop leading [Text] marker
    return _normalize_ws(s)

def clean_robust(raw) -> Tuple[str, Dict[str, Union[str, List[str]]]]:
    """
    Extract ALL SGML-ish blocks.
      - <TEXT> blocks become the main body (concatenate if multiple)
      - every other tag becomes metadata[tag] (string or list of strings)
    Anything not inside blocks is ignored by default
    """
    metadata: Dict[str, Any] = {}
    body_parts: List[str] = []
    if not raw:
        return "", metadata

    # Find all blocks
    for m in _BLOCK.finditer(raw):
        tag = m.group("tag").strip().upper()
        content = m.group("content") or ""
        cleaned = clean_inner_text(content)

        if not cleaned:
            continue

        if tag == "TEXT":
            body_parts.append(cleaned)
        else:
            # store possibly repeated tags as list
            if tag in metadata:
                if isinstance(metadata[tag], list):
                    metadata[tag].append(cleaned)
                else:
                    metadata[tag] = [metadata[tag], cleaned]
            else:
                metadata[tag] = cleaned

    # If there was no <TEXT> tag, fall back to cleaning the whole raw as body
    # (useful when some corpora omit TEXT)
    if not body_parts:
        # Remove all blocks completely, then clean what remains
        stripped = _BLOCK.sub("", raw)
        stripped = clean_inner_text(stripped)
        return stripped, metadata

    body = "\n\n".join(body_parts)
    return body, metadata

def split_passages(hits: List[Hit], splitter=SPLITTER_SINGLETON):
    return splitter.split_documents(hits)
